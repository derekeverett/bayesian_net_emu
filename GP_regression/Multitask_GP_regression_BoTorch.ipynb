{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Multitask GP model for Computer Model Emulation\n",
    "\n",
    "Based on this example: https://docs.gpytorch.ai/en/v1.2.0/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html\n",
    "\n",
    "### Import necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calculations_load import *\n",
    "from configurations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the directories for model design and outputs\n",
    "#run this cell only once, or else it will hang!\n",
    "#! sh prepare.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Computer Model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_str = 'Pb-Pb-2760'\n",
    "design_file = 'production_designs/500pts/design_pts_Pb_Pb_2760_production/design_points_main_PbPb-2760.dat'\n",
    "design = pd.read_csv(design_file)\n",
    "design = design.drop(\"idx\", axis=1)\n",
    "\n",
    "#delete bad design points\n",
    "drop_indices = list(delete_design_pts_set)\n",
    "design = design.drop(drop_indices)\n",
    "\n",
    "#choose features (inputs)\n",
    "#feature_cols = ['norm', 'trento_p'] #specific choices\n",
    "feature_cols = design.keys().values #all of them \n",
    "\n",
    "X = design[feature_cols]\n",
    "\n",
    "#perform a transformation of the design, swapping the original parameters\n",
    "#for inputs more naturally/simply related to the outputs\n",
    "do_transform_design = True\n",
    "if do_transform_design:\n",
    "    X = transform_design(X.values)\n",
    "    \n",
    "n_features = X.shape[1]\n",
    "\n",
    "n_design = SystemsInfo[\"Pb-Pb-2760\"][\"n_design\"]\n",
    "npt = n_design - len(delete_design_pts_set)\n",
    "\n",
    "Y = np.array([])\n",
    "Y_std = np.array([])\n",
    "for pt in range(npt):\n",
    "    for obs in active_obs_list['Pb-Pb-2760']:\n",
    "        Y = np.append( Y, trimmed_model_data[system_str][pt, idf][obs]['mean'][:], axis=0)\n",
    "        Y_std = np.append( Y_std, trimmed_model_data[system_str][pt, idf][obs]['err'][:], axis=0)\n",
    "        \n",
    "Y = Y.reshape(X.shape[0], -1)\n",
    "Y_std = Y_std.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape : (485, 29)\n",
      "Y.shape : (485, 110)\n"
     ]
    }
   ],
   "source": [
    "print( \"X.shape : \"+ str(X.shape) )\n",
    "print( \"Y.shape : \"+ str(Y.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the inputs and outputs into a training and testing set\n",
    "\n",
    "Then, scaling all of the inputs and outputs to (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
    "sss = ShuffleSplit(n_splits=1, test_size=0.2)\n",
    "sss.get_n_splits(X, Y)\n",
    "train_index, test_index = next(sss.split(X, Y)) \n",
    "\n",
    "X_train, X_test = X[train_index], X[test_index] \n",
    "Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "Y_std_train, Y_std_test = Y_std[train_index], Y_std[test_index]\n",
    "\n",
    "feature_range = (-1, 1)\n",
    "\n",
    "X_scaler = MinMaxScaler(feature_range = feature_range ).fit(X_train)\n",
    "Y_scaler = MinMaxScaler(feature_range = feature_range ).fit(Y_train)\n",
    "#X_scaler = StandardScaler().fit(X_train)\n",
    "#Y_scaler = StandardScaler().fit(Y_train)\n",
    "\n",
    "X_train_sc = X_scaler.transform(X_train)\n",
    "X_test_sc = X_scaler.transform(X_test)\n",
    "\n",
    "Y_train_sc = Y_scaler.transform(Y_train)\n",
    "Y_test_sc = Y_scaler.transform(Y_test)\n",
    "\n",
    "Y_std_train_sc = Y_std_train / Y_scaler.scale_\n",
    "\n",
    "#make the torch tensors\n",
    "X_train_sc = X_train_sc.astype(np.float32)\n",
    "Y_train_sc = Y_train_sc.astype(np.float32)\n",
    "Y_std_train_sc = Y_std_train_sc.astype(np.float32)\n",
    "X_train_tensor = torch.from_numpy(X_train_sc)\n",
    "Y_train_tensor = torch.from_numpy(Y_train_sc)\n",
    "Y_std_train_tensor = torch.from_numpy(Y_std_train_sc)\n",
    "\n",
    "X_test_sc = X_test_sc.astype(np.float32)\n",
    "Y_test_sc = Y_test_sc.astype(np.float32)\n",
    "X_test_tensor = torch.from_numpy(X_test_sc)\n",
    "Y_test_tensor = torch.from_numpy(Y_test_sc)\n",
    "\n",
    "\n",
    "#don't understand yet why we need to add the column for task features???\n",
    "X1, X2 = X_train_tensor[:, 0].reshape(-1, 1), X_train_tensor[:, 1].reshape(-1, 1)\n",
    "i1, i2 = torch.zeros(X1.shape[0], 1), torch.ones(X2.shape[0], 1)\n",
    "train_X = torch.cat([torch.cat([X1, i1], -1), torch.cat([X2, i2], -1),])\n",
    "\n",
    "for i in range(2, n_features):\n",
    "    X_feat = X_train_tensor[:, i].reshape(-1, 1)\n",
    "    indx   = i * torch.ones(X_feat.shape[0], 1)\n",
    "    train_X = torch.cat([ train_X , torch.cat([X_feat, indx], -1),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11252, 2])\n",
      "torch.Size([388, 110])\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(Y_std_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below constructs the Multitask GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Using BoTorch #######\n",
    "model = botorch.models.multitask.FixedNoiseMultiTaskGP(train_X, Y_train_tensor, \n",
    "                                                       Y_std_train_tensor, task_feature=-1)\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "#mll = fit_gpytorch_model(mll)\n",
    "###### Using BoTorch #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([388, 110])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (110) must match the size of tensor b (11252) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-90da6fae74a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iter %d/%d - Loss: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Get the log prob of the marginal distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Add additional terms (SGPR / learned inducing points, heteroskedastic likelihood models)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_covariance_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Repeat the covar to match the batch shape of diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (110) must match the size of tensor b (11252) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "training_iterations = 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "for i in range(training_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_X)\n",
    "    loss = -mll(output, Y_train_tensor)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = likelihood(model(X_test_tensor))\n",
    "    pred_mean = predictions.mean\n",
    "    pred_lower, pred_upper = predictions.confidence_region() #returns two sigma below, above mean\n",
    "    pred_std = (pred_upper - pred_lower) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.linspace(-1, 1, 100)\n",
    "\n",
    "sqrt_nplot = 4\n",
    "n_plot = sqrt_nplot * sqrt_nplot\n",
    "obs_indices = np.random.choice(110, n_plot, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(sqrt_nplot, sqrt_nplot, figsize=(4*sqrt_nplot, 4*sqrt_nplot))\n",
    "for i, obs_ind in enumerate(obs_indices):\n",
    "    ax = axes.flatten()[i]\n",
    "    ax.set_title(\"Obs \" + str(obs_ind))\n",
    "    ax.plot(truth, truth, c='r', lw=3, zorder=-1)\n",
    "    ax.errorbar(Y_test_sc[:, obs_ind], pred_mean[:, obs_ind], yerr=pred_std[:, obs_ind],\n",
    "                alpha=0.5, zorder=1, fmt='o')\n",
    "    \n",
    "    \n",
    "    r2 = r2_score(Y_test_sc[:, obs_ind], pred_mean[:, obs_ind])\n",
    "    ax.annotate(r'$r^2 = $' + str(round(r2, 3) ), xy = (-0.9, 0.9))\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "plt.tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_res_arr = []\n",
    "sigma_pred_arr = []\n",
    "\n",
    "for iobs in range(Y.shape[1]):\n",
    "    res = pred_mean[:, iobs] - Y_test_sc[:, iobs]\n",
    "    res = res.numpy()\n",
    "    sigma_res = np.std(res)\n",
    "    sigma_pred = np.mean(pred_std[:, iobs].numpy())\n",
    "    sigma_res_arr.append(sigma_res)\n",
    "    sigma_pred_arr.append(sigma_pred)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, Y.shape[1]), sigma_res_arr, lw=2, label='width of residual dist.')\n",
    "plt.plot(np.arange(0, Y.shape[1]), sigma_pred_arr, lw=2, color='r', label='pred. std', ls='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(Y_test_sc,pred_mean)\n",
    "print(\"r2 = \" + str(round(r2, 3)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
